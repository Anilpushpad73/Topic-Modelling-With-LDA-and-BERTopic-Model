# -*- coding: utf-8 -*-
"""Topic modelling_with_LDA_and_BERTopic_github .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K7C38d3yY49Twyp8FofDe96qyMVs1OQB

#### Contents
* introduction to topic modelling
* understanding gensim library
* Using Regular expressions
* Data Preprocessing (removal of emails and new lines nd formats)
* Bag of words concept
* Exploratory Data Analysis for text data
* Understanding Bigram and Trigram
* making LDA input and encoding of text data
* creating LDA models
* Selecting the number of topics
* Model Evaluation
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore", category = DeprecationWarning)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_json('/content/drive/MyDrive/newsgroups.json')
df



data = df.content.values.tolist()
data

"""## Data Cleaning
#### Removing the emails adresses
Replace :****
\S*@\S*\s?
by ''

Demo here

Some explanations :
\S* : match as many non-space characters you can
@ : then a @
\S* : then another sequence of non-space characters
\s? : And eventually a space, if there is one. Note that the '?' is needed to match an address at the end of the line. Because of the greediness of '?', if there is a space, it will always be matched.


"""

# Remove Emails
import re
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
data

# Remove new line characters
data = [re.sub('\s+', ' ', sent) for sent in data]

data = pd.Series(data)

# lets do some preproc3essing and move on
# here we are going to use regex for data cleaning
import re
data_processed = data.apply(lambda x: re.sub(r'[^a-zA-Z/s]+',' ',x).lower())
data_processed

import nltk
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('punkt')

# now lets deal with stopwords
# using nltk stopwords

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
len(stop_words)

from nltk import word_tokenize

data_final = data_processed.apply(lambda x:' '.join([word for word in word_tokenize(x) if word not in stop_words and len(word)> 2]))

data_final[0]

"""## Lets do some EDa"""

words_list = []
for sentence in data_final:
    words_list.extend(nltk.word_tokenize(sentence))
freq_dist = nltk.FreqDist(words_list)
freq_dist.most_common(20)
# freq_dist.keys()

# Commented out IPython magic to ensure Python compatibility.
# creating a temporary dataframe and plotting the graph
# %config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
temp = pd.DataFrame(freq_dist.most_common(50),  columns=['word', 'count'])
fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(x='word', y='count',
            data=temp, ax=ax, palette='rainbow')
plt.title("Top words")
plt.xticks(rotation='vertical')

# Annotate count of each word in a small and vertical form
for index, row in temp.iterrows():
    ax.text(index, row['count'] + 0.1, str(row['count']), color='black', ha="center", fontsize=8, rotation=90)

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'retina'

from wordcloud import WordCloud
import wordcloud
# creation of wordcloud
wcloud_fig = WordCloud( stopwords=set(wordcloud.STOPWORDS),
                      colormap='viridis', width=300, height=200).generate_from_frequencies(freq_dist)

# plotting the wordcloud
plt.figure(figsize=(10,7), frameon=True)

plt.imshow(wcloud_fig, interpolation  = 'bilinear')
plt.show()

"""# ## now lets create a Bigram and trigram for more sense

Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.

Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.

Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams.

"""

#loading libraries
import gensim
from gensim.models import Phrases
from gensim.models.phrases import Phraser
from gensim import models

words_sent = [x.split() for x in data_final]
words_sent
bigram = Phrases(words_sent, min_count=5, threshold=100)
trigram = Phrases(bigram[words_sent], threshold=100)

bigram_phraser = Phraser(bigram)
trigram_phraser = Phraser(trigram)

# bow = [bigram_phraser[word] for word in words_sent] # creating bigram
bow = [trigram_phraser[bigram_phraser[word]] for word in words_sent] # creating trigram and bigrambb

bow[0:1]

### lemmatisation by using spacy and pos tagging
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # disabing the parse and ner from pipeline it increase speed of pipeline

# here 'en' in spacy pipeline means the small version of language

!python -m spacy download en_core_web_sm

doc = nlp(' '.join(bow[0]))
for token in doc:
    print(token ,'=>', token.pos_)

# now lemmatising the whole corpus so that we can reach to the root words
def lemmatization(texts, tags=['NOUN', 'ADJ', 'VERB', 'ADV','PROPN']): # filter noun and adjective
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in tags])
    return texts_out

bag_of_words = lemmatization(bow)

bag_of_words[0:2]

flat_bag_of_words = [word for sublist in bag_of_words for word in sublist]

"""## now we need to encode the whole corpus into some numbers
The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them.
"""

from gensim import corpora
id2word = corpora.Dictionary(bag_of_words)
print(id2word)
# here keys are some numbers and values are our words

#  now lets create a encoded bag of words
corpus_matrix = [id2word.doc2bow(sent) for sent in bag_of_words]

corpus_matrix[0]

### Now create our model by using gensim lda

# Creating the object for LDA model using gensim library
import gensim
LDA = gensim.models.ldamodel.LdaModel

# Build LDA model
lda_model = LDA(corpus=corpus_matrix,id2word=id2word,
                                    num_topics=20,
                                    random_state=100,
                                           update_every=1,
                                           chunksize=150,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)

lda_model.print_topics()
# doc_lda = lda_model[corpus_matrix]

"""### Visualisation of topic modelling"""

! pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import pyLDAvis
import pyLDAvis.gensim
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Visualize the topics
fig, ax = plt.subplots(figsize=(10, 10))
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus = corpus_matrix, dictionary = id2word)
vis

"""#### Compute Model Perplexity and Coherence Score
Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been
"""

#  "Perplexity" is a common metric used to evaluate the performance of probabilistic topic models,
#  Perplexity measures how well  the model captures the underlying structure of the corpus.
#  Lower perplexity values indicate better performance.

#                N
# Perplexity=exp{−1/N ∑log p(wd)}
#               d=1

# N is the total number of words in the held-out documents.
# wd is the d-th document in the held-out set of doc.
# p(wd) is the probability assigned to the document wd by the model.

# Compute Coherence Score
from gensim.models import CoherenceModel

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus_matrix))
# a measure of how good the model is. lower the better.

# Compute Coherence Score
from gensim.models import CoherenceModel
coherence_model_lda = CoherenceModel(model=lda_model, texts=bag_of_words, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

"""####Finding the number of topics


Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.

If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.

The compute_coherence_values() (see below) trains multiple LDA models and provides the models and their corresponding coherence scores.

##Varying the number of passes while keeping the other hyperparameters constant and
these parameters are:
* NUMBER_TOPICS = 20
* ALPHA = 'auto' ( automatically learn which can improve the quality and interpretability)
* ETA = 'auto'
* CHUNK_SIZE = 150 ( number of documents processed in each training chunk)


As the number of passes increases, the model refines the topics by adjusting the word
distributions within topics. As the Coherence scores improve and typically reach a peak,
indicating the topics have become more semantically meaningful. But Identify the optimal
number of passes that balance between computational efficiency and topic quality i.e Maximize
coherence without unnecessary computation.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values1 = []
    model_list = []
    for num_of_passes in range(start, limit, step):
        model = LDA(corpus=corpus_matrix,id2word=id2word,
                                    num_topics=20,
                                    random_state=42,
                                           update_every=1,
                                           chunksize=100,
                                           passes=num_of_passes,
                                           alpha='auto',
                                            eta='auto',
                                           per_word_topics=True)
        # model = LDA(corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values1.append(coherencemodel.get_coherence())

    return model_list, coherence_values1

model_list, coherence_values1 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=45 ,start=5,  step=5)

coherence_values1

import matplotlib.pyplot as plt

limit=45; start=5; step=5;
# limit=350; start=50; step=50;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values1, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values1):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('number of passes vs Coherence Score')
plt.xlabel('number of passes')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""shows that among the evaluated number of passes, the coherence score
slightly fluctuates after 20 passes. It achieves the highest coherence score of
0.5400143812889777 at 30 passes. To avoid the limitations of overfitting risks and
excessive computation, the 30 passes strike an effective balance between representational
capacity and computational efficiency. Therefore, it is recommended as the suitable number of
passes for this application. However, we choose the optimal number of 20 passes for further
solutions to ensure maximum coherence score and computational efficiency.

---

##Varying the number of topic while keeping the other hyperparameters constant and
these parameters are:
* PASSES = 20
* CHUNK SIZE = 150
* ALPHA = ’auto’
* ETA = ’auto’

With a very low number of topics(<5) , the model struggles to capture the diversity of topics in
the documents and coherence scores are generally low because distinct topics are forced to be
traced as the same topic which decreases interpretability.
Beyond a certain point (>22), increasing the number of topics results in splitting coherent topics
into smaller, less meaningful subtopics. Coherence scores decrease, because the topics become
too specific and less interpretable. The model starts to capture noise and overfits the data,
reducing the overall quality of the topics.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values2 = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = LDA(corpus=corpus_matrix,id2word=id2word,
                                    num_topics=num_topics,
                                    random_state=42,
                                           update_every=1,
                                           chunksize=150,
                                           passes=20,
                                           alpha='auto',
                                            eta='auto',
                                           per_word_topics=True)
        # model = LDA(corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values2.append(coherencemodel.get_coherence())

    return model_list, coherence_values2

model_list, coherence_values2 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=30 ,start=1,  step=1)

coherence_values2

import matplotlib.pyplot as plt

limit=30; start=1;  step=1;
# limit=350; start=50; step=50;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values2, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values2):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('number of topics vs Coherence Score')
plt.xlabel('number of topics')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""shows that among the evaluated number of topics, the coherence score
fluctuates between 0.4207565431342476 and 0.6049269193617294, But after 19, number of
topics, the coherence score have increased the 22nd number of topics which is having
0.6049269193617294 maximum coherence score at 22nd number of topics. So we choose the
optimal number of 22 topics for further solutions to ensure maximum coherence score and
computational efficiency.

---

##Varying the chunk size while keeping the other hyperparameters constant and
These parameters are:
* NUMBER_TOPICS = 22
* PASSES = 20
* ALPHA = ’auto’
* ETA = ’auto’

At low chunk size, the coherence scores low due to frequent updates and hence, bring in
instability. Small chunks allow frequent updates, which can capture diverse patterns quickly but
also introduces noise then reducing coherence score, but increasing clock time. Large chunks
reduce the update frequency, which can slow convergence and lead to capturing less meaningful
patterns and decreasing coherence score.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=1, step=1):
    coherence_values3 = []
    model_list = []
    for chunk_size in range(start, limit, step):
        model = LDA(corpus=corpus_matrix,id2word=id2word,
                                    num_topics=22,
                                    random_state=42,
                                           update_every=1,
                                           chunksize=chunk_size,
                                           passes=20,
                                           alpha='auto',
                                            eta='auto',
                                           per_word_topics=True)
        # model = LDA(corpus=corpus, num_topics=num_topics, id2word=id2word)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values3.append(coherencemodel.get_coherence())

    return model_list, coherence_values3

model_list, coherence_values3 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=350 ,start=50,  step=50)

coherence_values3

import matplotlib.pyplot as plt

# limit=45; start=5; step=5;
limit=350; start=50; step=50;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values3, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values3):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('chunck size vs Coherence Score')
plt.xlabel('chunck size')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""we evaluate the coherence score against different chunk sizes. The
coherence score fluctuates within the range of 0.49589908184922275 to 0.64030572542999215
for chunk sizes between 50 and 300. The highest coherence score of 0.64030572542999215 is
observed at a chunk size of 100.

---

## from here we have got the best 22 topics to be selected in our data
"""

# Creating the object for LDA model using gensim library
import gensim
LDA = gensim.models.ldamodel.LdaModel

# Build LDA model
lda_model = LDA(corpus=corpus_matrix,id2word=id2word,
                                    num_topics=22,
                                    random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=20,
                                           alpha='auto',

                                           per_word_topics=True)

lda_model.print_topics()

# Visualize the topics
fig, ax = plt.subplots(figsize=(10, 10))
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus = corpus_matrix, dictionary = id2word)
vis

# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus_matrix))
# a measure of how good the model is. lower the better.

# Compute Coherence Score
from gensim.models import CoherenceModel
coherence_model_lda = CoherenceModel(model=lda_model, texts=bag_of_words, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

"""---

#Bertopic model##
"""

from IPython.display import Image
image_url = 'https://miro.medium.com/v2/resize:fit:1104/0*SleMld3TrABBzEfe.png'
width = 500
height = 500
Image(url=image_url, width=width, height=height)

pip install sentence-transformers

!pip install umap-learn

pip install hdbscan

pip install bertopic

from bertopic.representation import MaximalMarginalRelevance
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.representation import ZeroShotClassification
from bertopic.vectorizers import ClassTfidfTransformer
from sentence_transformers import SentenceTransformer
from bertopic.representation import KeyBERTInspired
from bertopic.representation import PartOfSpeech
from gensim.models import CoherenceModel
from transformers import BertTokenizer
from bertopic import BERTopic
import numpy as np
import hdbscan
import torch
import umap

# Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess(sentence):
    # Tokenize sentence and convert to input IDs
    input_ids = tokenizer.encode(sentence, add_special_tokens=True, truncation=True, max_length=64)
    # Create attention mask
    attention_mask = [1] * len(input_ids)
    return input_ids, attention_mask

# Preprocess sentences
preprocessed_sentences = [preprocess(sentence) for sentence in data_final]

import pandas as pd
df = pd.DataFrame(preprocessed_sentences, columns=['input_ids', 'attention_mask'])

df

# Extract input IDs and attention masks
input_ids = [preprocessed_sentence[0] for preprocessed_sentence in preprocessed_sentences]
attention_masks = [preprocessed_sentence[1] for preprocessed_sentence in preprocessed_sentences]

from torch.nn.utils.rnn import pad_sequence

# Convert to tensors
input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)
attention_masks = pad_sequence([torch.tensor(mask) for mask in attention_masks], batch_first=True, padding_value=0)

input_ids

len(input_ids[0])

attention_masks

image_url = 'https://miro.medium.com/v2/resize:fit:1400/1*1je5TwhVAwwnIeDFvww3ew.gif'
width = 700
height = 500
Image(url=image_url, width=width, height=height)

image_url = 'https://miro.medium.com/v2/resize:fit:664/1*tfYD93-NrGOTr6LG2WH3bQ.png'
width = 600
height = 500
Image(url=image_url, width=width, height=height)

"""**model_A: all-MiniLM-L6-v2**

"""

from sentence_transformers import SentenceTransformer
model_A = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings_A = model_A.encode(data_final)
print(embeddings_A)

"""**model_B: all-mpnet-base-v2**"""

model_B = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
embeddings_B = model_B.encode(data_final)
print(embeddings_B)

len(embeddings_B[0])

umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)
umap=umap_embeddings.fit_transform(embeddings_A)

umap

hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=10, alpha=1.0, metric='euclidean', cluster_selection_method='eom')
cluster_labels = hdbscan_model.fit_predict(umap)

# hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=10, cluster_selection_epsilon=0.5, metric='euclidean',
  #                             alpha=1.0, algorithm='best', leaf_size=40, n_jobs=-1, cluster_selection_method='eom')

  # cluster_labels = hdbscan_model.fit_predict(umap_embeddings)

print(cluster_labels)
# len(cluster)

vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english", min_df=5)
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

def apply_ctfidf(data, cluster_labels):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(data)

    tfidf_array = tfidf_matrix.toarray()
    c_tfidf_scores = []
    unique_labels = np.unique(cluster_labels)
    for label in unique_labels:

        class_indices = np.where(cluster_labels == label)[0]
        class_data = [data[i] for i in class_indices]

        # Calculate TF-IDF scores for the current class
        class_tfidf_matrix = tfidf_vectorizer.transform(class_data)
        class_tfidf_array = class_tfidf_matrix.toarray()

        # Calculate class-wise IDF
        class_idf = np.log(len(data) / (1 + np.sum(class_tfidf_array > 0, axis=0)))

        # Calculate c-TF-IDF scores
        c_tfidf_scores.append(np.mean(class_tfidf_array * class_idf, axis=0))

    return np.array(c_tfidf_scores)

# Assuming data_final and cluster_labels are defined somewhere
c_tfidf_scores = apply_ctfidf(data_final, cluster_labels)
print(c_tfidf_scores)

c_tfidf_scores[0].max()

# Relevance:  measured similarity score between  candidate item and the query. In the context of information retrieval, cosine similarity
# Diversity: This is measured by the dissimilarity between the selected item and the items already in the selected set.
# Score(item) = λ * Similarity(item, query) - (1 - λ) * max(Similarity(item, selected))

pos_patterns = [
            [{'POS': 'ADJ'}, {'POS': 'NOUN'}],
            [{'POS': 'NOUN'}], [{'POS': 'ADJ'}],
            [{'POS': 'VERB'}], [{'POS': 'ADV'}]
]
aspect_model1 = PartOfSpeech("en_core_web_sm",pos_patterns=pos_patterns)
aspect_model2 = MaximalMarginalRelevance(diversity=.5)
aspect_model3 = KeyBERTInspired()
# candidate_topics = ["space and nasa", "bicycles", "sports"]
# candidate_topics=key
# aspect_model4 = ZeroShotClassification(candidate_topics, model="facebook/bart-large-mnli")

representation_model = {
   "Aspect1":  aspect_model1,
   "Aspect2":  aspect_model2,
   "Aspect2":  aspect_model3,
  #  "Aspect4":  aspect_model4
}

#  vectorizer_model.set_params(min_df=1, max_df=1.0)
topic_model = BERTopic(
    nr_topics=20,
    embedding_model=model_A,
    umap_model=umap_embeddings,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    representation_model=representation_model
)

topics, probs = topic_model.fit_transform(data_final)

topic_info=topic_model.get_topic_info()

topics = topic_model.get_topics()
topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]
coherence_values = []
# Calculate the coherence score
coherencemodel = CoherenceModel(
    topics=topic_words,
    texts=bag_of_words,
    dictionary=id2word,
    coherence='c_v')

coherence_values.append(coherencemodel.get_coherence())
coherence_values

topics

topic_model.visualize_heatmap()

from sklearn.metrics.pairwise import cosine_similarity
distance_matrix = cosine_similarity(np.array(topic_model.topic_embeddings_))
dist_df = pd.DataFrame(distance_matrix, columns=topic_model.topic_labels_.values(),
                       index=topic_model.topic_labels_.values())

tmp = []
for rec in dist_df.reset_index().to_dict('records'):
    t1 = rec['index']
    for t2 in rec:
        if t2 == 'index':
            continue
        tmp.append(
            {
                'topic1': t1,
                'topic2': t2,
                'distance': rec[t2]
            }
        )

pair_dist_df = pd.DataFrame(tmp)

pair_dist_df = pair_dist_df[(pair_dist_df.topic1.map(
      lambda x: not x.startswith('-1'))) &
            (pair_dist_df.topic2.map(lambda x: not x.startswith('-1')))]
pair_dist_df = pair_dist_df[pair_dist_df.topic1 < pair_dist_df.topic2]
pair_dist_df.sort_values('distance', ascending = False).head(20)

topic_model.visualize_topics()

# Visualize top topic keywords
topic_model.visualize_barchart(top_n_topics=12)

"""---

##Varying the number of neighbors while keeping the other hyperparameters constant
and these parameters are:
* ➢ Using the all-MiniLM-L6-v2 embeddings model
* NUMBER_TOPICS = 20
* MIN_CLUSTER_SIZE = 10
* MIN_SAMPLES = 10
* ALPHA = ’none’

Number of neighbors determines the number of neighboring points used for local approximation
in the manifold learning process.The number of neighbors (n_neighbors) affects the topic
coherence score. With a small n_neighbors, the model captures too much noise and focuses on
local details, missing the bigger picture and resulting in unclear topics so that shows the low
coherence score. With a large n_neighbors, the model captures broad patterns but misses
important details. This makes topics too general and less useful and thus decreases the coherence
score again. So Balancing n_neighbors is crucial for capturing both detailed and overall patterns,
leading to better coherence scores and more understandable topics
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5, min_df=1, max_df=1.0):
    coherence_values4 = []
    model_list4 = []
    num_topics_list4 = []

    for neighbors in np.arange(start, limit, step):
        umap_embeddings = umap.UMAP(n_neighbors=neighbors,
                            n_components=2,
                            metric='euclidean',random_state=42)
        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=20,
            embedding_model=model_A,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data (assuming 'data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values4.append(coherencemodel.get_coherence())
        model_list4.append(topic_model)
        num_topics_list4.append(len(topics))

    return model_list4, coherence_values4, num_topics_list4

model_list4, coherence_values4, num_topics_list4 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=50, start=5, step=5)

coherence_values4

import matplotlib.pyplot as plt
limit=50; start=5; step=5;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values4, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values4):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title(' number of neighbors vs Coherence Score')
plt.xlabel('number of neighbors')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""we evaluate the coherence score against different numbers of neighbors .
The coherence score fluctuates within the range of 0.48582252434505047 to 0.55294548912511
for neighbors between 5 and 45. The highest coherence score of 0.55294548912511 is observed
at the neighbor 15. So we choose the optimal number of 15 neighbors for further solutions to
ensure maximum coherence score and computational efficiency.

---

##Varying the minimum cluster size while keeping the other hyperparameters constant
and these parameters are:
* ➢ Using the all-MiniLM-L6-v2 embeddings model
* NUMBER_TOPICS = 20
* N_NEIGHBORS = 15
* MIN_SAMPLES = 10
* ALPHA = ’none’

The minimum cluster size determines the smallest group of points that can be considered a
cluster. Initially, having a small minimum cluster size may result in low and fluctuating
coherence scores, as small clusters can capture minor variations and noise, thereby reducing
topic coherence. Conversely, with a large minimum cluster size, coherence scores may plateau or
decrease because larger clusters tend to generalize too broadly, reducing topic coherence.
Therefore, we select an optimal minimum cluster size to ensure that HDBSCAN identifies
meaningful topics without being overly specific or broad, leading to higher coherence scores.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5,min_df=1, max_df=1.0):
    coherence_values5 = []
    model_list5 = []
    num_topics_list5 = []

    for cluster_size in range(start, limit, step):
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=cluster_size, min_samples=10, metric='euclidean', cluster_selection_method='eom')
        umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)

        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=20,
            embedding_model=model_A,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data (assuming 'data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values5.append(coherencemodel.get_coherence())
        model_list5.append(topic_model)
        num_topics_list5.append(len(topics))

    return model_list5, coherence_values5, num_topics_list5

model_list5, coherence_values5, num_topics_list5 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=110, start=10, step=10)

coherence_values5

import matplotlib.pyplot as plt

# limit=45; start=5; step=5;
limit=110; start=10; step=10;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values5, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values5):
    plt.plot([x, x], [0.5, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title(' min_cluster_size vs Coherence Score')
plt.xlabel('min_cluster_size')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""we evaluate the coherence score against different minimum cluster size.
The coherence score fluctuates within the range of 0.55294548912511 to
0.6472625012887551 for minimum cluster size between 10 and 100. The highest coherence
score of 0.6472625012887551 is observed at a minimum cluster size of 60. So we choose the
optimal number of 60 minimum cluster size for further solutions to ensure maximum coherence
score and computational efficiency.

---

##Varying the minimum sample while keeping the other hyperparameters constant and
these parameters are:
* ➢ Using the all-MiniLM-L6-v2 embeddings model
* NUMBER_TOPICS = 20
* N_NEIGHBORS = 15
* MIN_CLUSTER_SIZE = 60
* ALPHA = ’none’

The minimum sample (min_samples) determines the minimum sample size, which is crucial as it
determines the minimum number of points required to form a dense region, which affects the
definition of noise. The larger the min_samples value, the more conservative the clustering
becomes, as more points are declared as noise and the clusters are restricted to denser regions.
This affects the coherence values, as small min_samples may contain too much noise, which
reduces coherence, while large min_samples may overgeneralize the clusters, which also reduces
coherence values. We choose Balancing min_samples to get better coherence score and more
interpretable topics.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5,min_df=1, max_df=1.0):
    coherence_values6 = []
    model_list6 = []
    num_topics_list6 = []

    for sample_size in range(start, limit, step):
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=60, min_samples=sample_size, metric='euclidean', cluster_selection_method='eom')
        umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)

        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=20,
            embedding_model=model_A,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data (assuming 'data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values6.append(coherencemodel.get_coherence())
        model_list6.append(topic_model)
        num_topics_list6.append(len(topics))

    return model_list6, coherence_values6, num_topics_list6

model_list6, coherence_values6, num_topics_list6 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=20, start=1, step=2)

coherence_values6

import matplotlib.pyplot as plt


limit=20; start=1; step=2;
topic = range(start, limit, step)
# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values6, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values6):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title(' minimum sample size vs Coherence Score')
plt.xlabel('minimum sample size')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""we evaluate the coherence score against different minimum samples. The
coherence score fluctuates within the range of 0.5934293650110775 to 0.7025142070734569
for minimum sample size between 1 and 20. The highest coherence score of
0.7025142070734569 is observed at a minimum sample size of 1. So we choose the optimal
number of 1 minimum sample size for further solutions to ensure maximum coherence score and
more interpretable topics.

---

##Varying the Alpha while keeping the other hyperparameters constant and
these parameters are:
* MIN_SAMPLES = 1
* N_NEIGHBORS = 15
* MIN_CLUSTER_SIZE = 60
* NUM OF TOPIC=20
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5, min_df=1, max_df=1.0):
    coherence_values7 = []
    model_list7 = []
    num_topics_list7 = []

    for nalpha in np.arange(start, limit, step):
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=60, min_samples=1, alpha=nalpha, metric='euclidean', cluster_selection_method='eom')
        umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)

        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=20,
            embedding_model=model_A,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data (assuming 'data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values7.append(coherencemodel.get_coherence())
        model_list7.append(topic_model)
        num_topics_list7.append(len(topics))

    return model_list7, coherence_values7, num_topics_list7

model_list7, coherence_values7, num_topics_list7 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=1.05, start=0.0, step=0.05)

coherence_values7

import numpy as np
import matplotlib.pyplot as plt

# Parameters for the range
limit = 1.05
start = 0.0
step = 0.05

# Generate the sequence of floating-point numbers
topic = np.arange(start, limit, step)


# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values7, marker='o', linestyle='-', color='b')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values7):
    plt.plot([x, x], [0.5, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0.8, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('alpha vs Coherence Score')
plt.xlabel('alpha')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""---

There are using two different embedding model to compare the results:

* **Model A:** The **all-MiniLM-L6-v2** embedding model is designed to balance speed and
performance. It has 6 layers and uses 384-dimensional vectors to represent text. This
model is efficient, providing decent semantic quality while being relatively lightweight,
making it faster for processing large datasets.
* **Model B:** The **all-mpnet-base-v2** embedding model is more advanced, offering richer
and more nuanced semantic embeddings. It uses 12 layers and 768-dimensional vectors,
providing high-quality embeddings that capture complex relationships in the data

When using the all-MiniLM-L6-v2 embedding model, coherence scores start low with few broad
topics. As the number of topics increases to a moderate range, coherence scores improve due to
the model's balanced semantic representation, resulting in clearer and more meaningful topics.
However, with too many topics, coherence scores plateau or decline as topics become overly
specific and fragmented coherent topics. In contrast, the all-mpnet-base-v2 model, with its richer
and nuanced semantic embeddings, shows a similar trend but maintains higher coherence scores
longer, providing clearer and more interpretable topics before also facing a plateau or decline
with excessive topics

##Varying the number of topics while keeping the other hyperparameters constant and
these parameters are:
* ➢ Using the all-MiniLM-L6-v2 embeddings model
* MIN_SAMPLES = 1
* N_NEIGHBORS = 15
* MIN_CLUSTER_SIZE = 60
* ALPHA = 1.0 (for all value of alpha give the same coherence score)

Initially, with few topics, coherence scores are low because each topic covers a wide range of
concepts, making them less clear. As the number of topics increases to an optimal range, the coherence scores improve because the topics become more focused and meaningful. However, if
the number of topics increases too much, the coherence scores stagnate or decrease because the
topics become too specific and coherent topics are split into smaller, which become less
meaningful subtopics, creating noise.
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5, min_df=1, max_df=1.0):
    coherence_values8 = []
    model_list8 = []
    num_topics_list8 = []

    for n_topics in np.arange(start, limit, step):
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=60, min_samples=1, alpha=1, metric='euclidean', cluster_selection_method='eom')
        umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)

        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=n_topics,
            embedding_model=model_A,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data (assuming 'data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values8.append(coherencemodel.get_coherence())
        model_list8.append(topic_model)
        num_topics_list8.append(len(topics))

    return model_list8, coherence_values8, num_topics_list8

model_list8, coherence_values8, num_topics_list8 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=32, start=2, step=2)

coherence_values8

import numpy as np
import matplotlib.pyplot as plt

# Parameters for the range
limit = 32
start = 2
step = 2

# Generate the sequence of floating-point numbers
topic = np.arange(start, limit, step)


# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values8, marker='o', linestyle='-', color='b',label='Model A')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values8):
    plt.plot([x, x], [0.5, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0.8, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('Number of Topics vs Coherence Score( Model A)')
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

"""---

**Varying the number of topics while keeping the other hyperparameters constant and**

these parameters are:

* ➢ Using the all-mpnet-base-v2 embeddings model
* MIN_SAMPLES = 1
* N_NEIGHBORS = 15
* MIN_CLUSTER_SIZE = 60
* ALPHA = 1.0 (for all value of alpha give the same coherence score)
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=5, step=5, min_df=1, max_df=1.0):
    coherence_values9 = []
    model_list9 = []
    num_topics_list9 = []

    for n_topics in np.arange(start, limit, step):
        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=60, min_samples=1, alpha=1, metric='euclidean', cluster_selection_method='eom')
        umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=2,
                            metric='euclidean',random_state=42)

        vectorizer_model.set_params(min_df=min_df, max_df=max_df)
        topic_model = BERTopic(
            nr_topics=n_topics,
            embedding_model=model_B,
            umap_model=umap_embeddings,
            hdbscan_model=hdbscan_model,
            vectorizer_model=vectorizer_model,
            ctfidf_model=ctfidf_model,
            representation_model=representation_model
        )

        # Fit the BERTopic model to your data ('data' is a list of documents)
        topics, probs = topic_model.fit_transform(data)

        # Extract the topic words
        topics = topic_model.get_topics()
        topic_words = [[word for word, _ in topic] for topic in topics.values() if topic]

        # Calculate the coherence score
        coherencemodel = CoherenceModel(
            topics=topic_words,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )

        coherence_values9.append(coherencemodel.get_coherence())
        model_list9.append(topic_model)
        num_topics_list9.append(len(topics))

    return model_list9, coherence_values9, num_topics_list9

model_list9, coherence_values9, num_topics_list9 = compute_coherence_values(dictionary=id2word, corpus=corpus_matrix, texts=bag_of_words, limit=32, start=2, step=2)

coherence_values9

import numpy as np
import matplotlib.pyplot as plt

# Parameters for the range
limit = 32
start = 2
step = 2

# Generate the sequence of floating-point numbers
topic = np.arange(start, limit, step)


# Create the plot
plt.figure(figsize=(10, 6))
plt.plot(topic, coherence_values9, marker='o', linestyle='-', color='b',label='Model B')

# Add dotted lines for each point
for x, y in zip(topic, coherence_values9):
    plt.plot([x, x], [0.5, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0.8, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('Number of Topics vs Coherence Score( Model B)')
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Score')

# Show the plot
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Parameters for the range of topics
limit = 32
start = 2
step = 2
topics = range(start, limit, step)

# Create the plot
plt.figure(figsize=(10, 6))

# Plot for Model A
plt.plot(topics, coherence_values8, marker='*', linestyle='-', color='b', label='Model A')

# Add dotted lines for Model A
for x, y in zip(topics, coherence_values8):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Plot for Model B
plt.plot(topics, coherence_values9, marker='.', linestyle='-', color='r', label='Model B')

# Add dotted lines for Model B
for x, y in zip(topics, coherence_values9):
    plt.plot([x, x], [0.4, y], linestyle='--', color='gray', linewidth=0.5)
    plt.plot([0, x], [y, y], linestyle='--', color='gray', linewidth=0.5)

# Add title and labels
plt.title('Number of Topics vs Coherence Score')
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Score')

# Add legend
plt.legend()

# Show the plot
plt.grid(True)
plt.show()

"""we evaluate the coherence score as a function of the number of topics
with different embedding models. For model A, the coherence score varies between
0.4625748773569187 and 0.7202945489125116 and for model B, the coherence score varies
between 0.49594481921233756 and 0.7537944683203674 for the number of topics between 1
and 30. The highest coherence score of 0.7202945489125116 is observed for the number of
topics of model A with 18, while for model B, the highest coherence score of
0.7537944683203674 is observed for the number of topics of 22.

---
"""

topic_model = BERTopic(nr_topics = 'auto',embedding_model=model_B,
                       umap_model=umap_embeddings,
                       hdbscan_model=hdbscan_model,
                       vectorizer_model=vectorizer_model,
                       ctfidf_model=ctfidf_model,
                       representation_model=representation_model)

topics, probs = topic_model.fit_transform(data_final)

topic=topic_model.get_topic_info()

topic

topic_model.visualize_heatmap()

from sklearn.metrics.pairwise import cosine_similarity
distance_matrix = cosine_similarity(np.array(topic_model.topic_embeddings_))
dist_df = pd.DataFrame(distance_matrix, columns=topic_model.topic_labels_.values(),
                       index=topic_model.topic_labels_.values())

tmp = []
for rec in dist_df.reset_index().to_dict('records'):
    t1 = rec['index']
    for t2 in rec:
        if t2 == 'index':
            continue
        tmp.append(
            {
                'topic1': t1,
                'topic2': t2,
                'distance': rec[t2]
            }
        )

pair_dist_df = pd.DataFrame(tmp)

pair_dist_df = pair_dist_df[(pair_dist_df.topic1.map(
      lambda x: not x.startswith('-1'))) &
            (pair_dist_df.topic2.map(lambda x: not x.startswith('-1')))]
pair_dist_df = pair_dist_df[pair_dist_df.topic1 < pair_dist_df.topic2]
pair_dist_df.sort_values('distance', ascending = False).head(20)

# Visualize top topic keywords
topic_model.visualize_barchart(top_n_topics=12)

topic_model.visualize_topics()

topics_list = topic_model.get_topics()

topics_list

topics_list[1]

# Visualize connections between topics using hierachical clustering
topic_model.visualize_hierarchy(top_n_topics=22)

"""
# part 1
all-MiniLM-L6-v2'"""

topic_model.visualize_heatmap()



from sklearn.metrics.pairwise import cosine_similarity
distance_matrix = cosine_similarity(np.array(topic_model.topic_embeddings_))
dist_df = pd.DataFrame(distance_matrix, columns=topic_model.topic_labels_.values(),
                       index=topic_model.topic_labels_.values())

tmp = []
for rec in dist_df.reset_index().to_dict('records'):
    t1 = rec['index']
    for t2 in rec:
        if t2 == 'index':
            continue
        tmp.append(
            {
                'topic1': t1,
                'topic2': t2,
                'distance': rec[t2]
            }
        )

pair_dist_df = pd.DataFrame(tmp)

pair_dist_df = pair_dist_df[(pair_dist_df.topic1.map(
      lambda x: not x.startswith('-1'))) &
            (pair_dist_df.topic2.map(lambda x: not x.startswith('-1')))]
pair_dist_df = pair_dist_df[pair_dist_df.topic1 < pair_dist_df.topic2]
pair_dist_df.sort_values('distance', ascending = False).head(20)

"""# part 2
all-mpnet-base-v2'
"""

topic_model.visualize_heatmap()

from sklearn.metrics.pairwise import cosine_similarity
distance_matrix = cosine_similarity(np.array(topic_model.topic_embeddings_))
dist_df = pd.DataFrame(distance_matrix, columns=topic_model.topic_labels_.values(),
                       index=topic_model.topic_labels_.values())

tmp = []
for rec in dist_df.reset_index().to_dict('records'):
    t1 = rec['index']
    for t2 in rec:
        if t2 == 'index':
            continue
        tmp.append(
            {
                'topic1': t1,
                'topic2': t2,
                'distance': rec[t2]
            }
        )

pair_dist_df = pd.DataFrame(tmp)

pair_dist_df = pair_dist_df[(pair_dist_df.topic1.map(
      lambda x: not x.startswith('-1'))) &
            (pair_dist_df.topic2.map(lambda x: not x.startswith('-1')))]
pair_dist_df = pair_dist_df[pair_dist_df.topic1 < pair_dist_df.topic2]
pair_dist_df.sort_values('distance', ascending = False).head(20)

# Visualize top topic keywords
topic_model.visualize_barchart(top_n_topics=12)

topic_model.visualize_topics()